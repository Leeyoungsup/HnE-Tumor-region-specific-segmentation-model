{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# Breast Tumor Segmentation Model - Prediction & Evaluation\n",
    "\n",
    "This notebook evaluates the trained DeepLabV3+ model for breast tumor region segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/urban/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except:\n",
    "    print(\"seaborn not available, using matplotlib only.\")\n",
    "    sns = None\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "batch_size = 2\n",
    "img_size = 512\n",
    "class_list = [\n",
    "    \"Background\",\n",
    "    \"Stroma\",\n",
    "    \"Non_Tumor\",\n",
    "    \"Tumor\",\n",
    "]\n",
    "\n",
    "# Paths\n",
    "img_path = '../../data/NIPA/integrated_512/breast/images/'\n",
    "model_path = '../../model/Tumor_region_segmentation/breast/BR_callback.pt'\n",
    "\n",
    "# Color map for visualization\n",
    "color_map = {\n",
    "    0: (255, 255, 255),  # Background - White\n",
    "    1: (0, 255, 0),      # Stroma - Green\n",
    "    2: (0, 0, 255),      # Non_Tumor - Blue\n",
    "    3: (255, 255, 0),    # Tumor - Yellow\n",
    "}\n",
    "\n",
    "tf = ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-md",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "        self.label = label_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.img_path[idx]\n",
    "        label = self.label[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 10625\n",
      "Train images: 8500\n",
      "Test images: 2125\n"
     ]
    }
   ],
   "source": [
    "# Load image and mask paths\n",
    "img_list = glob(img_path + '*.jpeg')\n",
    "mask_list = [i.replace('/images/', '/masks/') for i in img_list]\n",
    "mask_list = [i.replace('.jpeg', '.npy') for i in mask_list]\n",
    "\n",
    "# Train/Test split (same random_state as training)\n",
    "train_img_list, test_img_list, train_mask_list, test_mask_list = train_test_split(\n",
    "    img_list, mask_list, test_size=0.2, random_state=41\n",
    ")\n",
    "\n",
    "print(f\"Total images: {len(img_list)}\")\n",
    "print(f\"Train images: {len(train_img_list)}\")\n",
    "print(f\"Test images: {len(test_img_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-test-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 148/2125 [00:20<04:29,  7.33it/s]"
     ]
    }
   ],
   "source": [
    "# Load test data into memory\n",
    "print(\"Loading test data...\")\n",
    "test_img_resource_list = torch.zeros((len(test_img_list), 3, img_size, img_size))\n",
    "test_mask_resource_list = torch.zeros((len(test_img_list), len(class_list), img_size, img_size))\n",
    "\n",
    "for i in tqdm(range(len(test_img_list))):\n",
    "    image = tf(Image.open(test_img_list[i]).resize((img_size, img_size)))\n",
    "    mask = torch.from_numpy(np.load(test_mask_list[i])).float()\n",
    "    mask = mask.permute(2, 0, 1)\n",
    "    test_img_resource_list[i] = image\n",
    "    test_mask_resource_list[i] = mask\n",
    "\n",
    "test_dataset = CustomDataset(test_img_resource_list, test_mask_resource_list)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Test dataloader batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-loading-md",
   "metadata": {},
   "source": [
    "## 2. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"efficientnet-b5\",\n",
    "    encoder_weights=None,\n",
    "    in_channels=3,\n",
    "    classes=len(class_list),\n",
    ").to(device)\n",
    "\n",
    "# Load trained weights\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "print(f\"Model loaded from: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-md",
   "metadata": {},
   "source": [
    "## 3. Sample Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(model, dataloader, num_samples=3):\n",
    "    \"\"\"Visualize sample predictions\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_iter = iter(dataloader)\n",
    "        \n",
    "        for sample_idx in range(num_samples):\n",
    "            try:\n",
    "                x_test, y_test = next(test_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            \n",
    "            x_test = x_test.to(device).float()\n",
    "            y_test = y_test.to(device).float()\n",
    "            \n",
    "            predict = model(x_test)\n",
    "            predict_softmax = F.softmax(predict, dim=1)\n",
    "            predict_argmax = torch.argmax(predict_softmax, dim=1)\n",
    "            \n",
    "            x_test_np = x_test.cpu().numpy()\n",
    "            y_test_np = y_test.cpu().numpy()\n",
    "            predict_np = predict_argmax.cpu().numpy()\n",
    "            \n",
    "            batch_idx = 0\n",
    "            original_img = np.transpose(x_test_np[batch_idx], (1, 2, 0))\n",
    "            \n",
    "            # Ground Truth mask\n",
    "            gt_mask_rgb = np.zeros((img_size, img_size, 3), dtype=np.uint8)\n",
    "            gt_argmax = np.argmax(y_test_np[batch_idx], axis=0)\n",
    "            for class_idx, color in color_map.items():\n",
    "                gt_mask_rgb[gt_argmax == class_idx] = color\n",
    "            \n",
    "            # Prediction mask\n",
    "            pred_mask_rgb = np.zeros((img_size, img_size, 3), dtype=np.uint8)\n",
    "            for class_idx, color in color_map.items():\n",
    "                pred_mask_rgb[predict_np[batch_idx] == class_idx] = color\n",
    "            \n",
    "            # Overlay\n",
    "            alpha = 0.4\n",
    "            overlay_img = (original_img * 255).astype(np.uint8).copy()\n",
    "            for class_idx, color in color_map.items():\n",
    "                if class_idx == 0:\n",
    "                    continue\n",
    "                mask_area = (predict_np[batch_idx] == class_idx)\n",
    "                if mask_area.any():\n",
    "                    color_array = np.array(color, dtype=np.uint8)\n",
    "                    overlay_img[mask_area] = ((1 - alpha) * overlay_img[mask_area] + alpha * color_array).astype(np.uint8)\n",
    "            \n",
    "            # Plot\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "            \n",
    "            axes[0].imshow(original_img)\n",
    "            axes[0].set_title('Original Image', fontsize=14)\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            axes[1].imshow(gt_mask_rgb)\n",
    "            axes[1].set_title('Ground Truth', fontsize=14)\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            axes[2].imshow(pred_mask_rgb)\n",
    "            axes[2].set_title('Prediction', fontsize=14)\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            axes[3].imshow(overlay_img)\n",
    "            axes[3].set_title('Original + Prediction Overlay', fontsize=14)\n",
    "            axes[3].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "visualize_prediction(model, test_dataloader, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show color legend\n",
    "plt.figure(figsize=(12, 2))\n",
    "for i, (class_idx, color) in enumerate(color_map.items()):\n",
    "    plt.barh(0, 1, left=i, color=np.array(color)/255.0, edgecolor='black')\n",
    "    plt.text(i+0.5, 0, class_list[class_idx], ha='center', va='center', \n",
    "            rotation=45, fontsize=10)\n",
    "\n",
    "plt.xlim(0, len(color_map))\n",
    "plt.ylim(-0.5, 0.5)\n",
    "plt.title('Class Color Map', fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-md",
   "metadata": {},
   "source": [
    "## 4. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dice_coefficient(pred, target, smooth=1e-6):\n",
    "    \"\"\"Calculate Dice coefficient\"\"\"\n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "    return dice\n",
    "\n",
    "def calculate_iou(pred, target, smooth=1e-6):\n",
    "    \"\"\"Calculate IoU (Jaccard Index)\"\"\"\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou\n",
    "\n",
    "def calculate_95_ci(data):\n",
    "    \"\"\"Calculate 95% confidence interval\"\"\"\n",
    "    mean = np.mean(data)\n",
    "    if len(data) > 1:\n",
    "        se = stats.sem(data)\n",
    "        ci = stats.t.interval(0.95, len(data)-1, loc=mean, scale=se)\n",
    "    else:\n",
    "        ci = (mean, mean)\n",
    "    return mean, ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full test set evaluation\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_dice_scores = {i: [] for i in range(len(class_list))}\n",
    "all_iou_scores = {i: [] for i in range(len(class_list))}\n",
    "pixel_accuracies = []\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "with torch.no_grad():\n",
    "    for batch_idx_eval, (x_test_eval, y_test_eval) in enumerate(tqdm(test_dataloader)):\n",
    "        x_test_eval = x_test_eval.to(device).float()\n",
    "        y_test_eval = y_test_eval.to(device).float()\n",
    "        \n",
    "        predict_eval = model(x_test_eval)\n",
    "        predict_softmax_eval = F.softmax(predict_eval, dim=1)\n",
    "        predict_argmax_eval = torch.argmax(predict_softmax_eval, dim=1)\n",
    "        \n",
    "        y_test_eval_np = y_test_eval.cpu().numpy()\n",
    "        predict_eval_np = predict_argmax_eval.cpu().numpy()\n",
    "        \n",
    "        for i in range(predict_eval_np.shape[0]):\n",
    "            y_true = np.argmax(y_test_eval_np[i], axis=0)\n",
    "            y_pred = predict_eval_np[i]\n",
    "            \n",
    "            all_predictions.extend(y_pred.flatten())\n",
    "            all_targets.extend(y_true.flatten())\n",
    "            \n",
    "            pixel_acc = accuracy_score(y_true.flatten(), y_pred.flatten())\n",
    "            pixel_accuracies.append(pixel_acc)\n",
    "            \n",
    "            for class_idx in range(len(class_list)):\n",
    "                y_true_class = (y_true == class_idx).astype(np.float32)\n",
    "                y_pred_class = (y_pred == class_idx).astype(np.float32)\n",
    "                \n",
    "                dice = calculate_dice_coefficient(y_pred_class, y_true_class)\n",
    "                iou = calculate_iou(y_pred_class, y_true_class)\n",
    "                \n",
    "                all_dice_scores[class_idx].append(dice)\n",
    "                all_iou_scores[class_idx].append(iou)\n",
    "\n",
    "print(\"Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BREAST TUMOR SEGMENTATION - TEST SET PERFORMANCE EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Pixel Accuracy\n",
    "pixel_acc_mean, pixel_acc_ci = calculate_95_ci(pixel_accuracies)\n",
    "print(f\"\\n1. Pixel Accuracy:\")\n",
    "print(f\"   Mean ± Std: {pixel_acc_mean:.4f} ± {np.std(pixel_accuracies):.4f}\")\n",
    "print(f\"   95% CI: [{pixel_acc_ci[0]:.4f}, {pixel_acc_ci[1]:.4f}]\")\n",
    "print(f\"   Min: {np.min(pixel_accuracies):.4f}, Max: {np.max(pixel_accuracies):.4f}\")\n",
    "\n",
    "# 2. Dice Score per Class\n",
    "print(f\"\\n2. Dice Score per Class:\")\n",
    "print(f\"{'Class':<15} {'Mean':<10} {'Std':<10} {'95% CI':<25} {'Min':<10} {'Max':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "dice_means = []\n",
    "for class_idx in range(len(class_list)):\n",
    "    dice_mean, dice_ci = calculate_95_ci(all_dice_scores[class_idx])\n",
    "    dice_std = np.std(all_dice_scores[class_idx])\n",
    "    dice_min = np.min(all_dice_scores[class_idx])\n",
    "    dice_max = np.max(all_dice_scores[class_idx])\n",
    "    dice_means.append(dice_mean)\n",
    "    \n",
    "    print(f\"{class_list[class_idx]:<15} {dice_mean:<10.4f} {dice_std:<10.4f} [{dice_ci[0]:.4f}, {dice_ci[1]:.4f}]  {dice_min:<10.4f} {dice_max:<10.4f}\")\n",
    "\n",
    "overall_dice_mean = np.mean(dice_means)\n",
    "overall_dice_std = np.std(dice_means)\n",
    "print(f\"\\nMean Dice Score (mDSC): {overall_dice_mean:.4f} ± {overall_dice_std:.4f}\")\n",
    "\n",
    "# 3. IoU per Class\n",
    "print(f\"\\n3. IoU (Jaccard Index) per Class:\")\n",
    "print(f\"{'Class':<15} {'Mean':<10} {'Std':<10} {'95% CI':<25} {'Min':<10} {'Max':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "iou_means = []\n",
    "for class_idx in range(len(class_list)):\n",
    "    iou_mean, iou_ci = calculate_95_ci(all_iou_scores[class_idx])\n",
    "    iou_std = np.std(all_iou_scores[class_idx])\n",
    "    iou_min = np.min(all_iou_scores[class_idx])\n",
    "    iou_max = np.max(all_iou_scores[class_idx])\n",
    "    iou_means.append(iou_mean)\n",
    "    \n",
    "    print(f\"{class_list[class_idx]:<15} {iou_mean:<10.4f} {iou_std:<10.4f} [{iou_ci[0]:.4f}, {iou_ci[1]:.4f}]  {iou_min:<10.4f} {iou_max:<10.4f}\")\n",
    "\n",
    "overall_iou_mean = np.mean(iou_means)\n",
    "overall_iou_std = np.std(iou_means)\n",
    "print(f\"\\nMean IoU (mIoU): {overall_iou_mean:.4f} ± {overall_iou_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "print(\"\\n4. Confusion Matrix:\")\n",
    "cm = confusion_matrix(all_targets, all_predictions)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw counts\n",
    "if sns:\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_list, yticklabels=class_list, ax=axes[0])\n",
    "else:\n",
    "    im = axes[0].imshow(cm, cmap='Blues')\n",
    "    for i in range(len(class_list)):\n",
    "        for j in range(len(class_list)):\n",
    "            axes[0].text(j, i, f'{cm[i, j]}', ha='center', va='center')\n",
    "    axes[0].set_xticks(range(len(class_list)))\n",
    "    axes[0].set_yticks(range(len(class_list)))\n",
    "    axes[0].set_xticklabels(class_list, rotation=45)\n",
    "    axes[0].set_yticklabels(class_list)\n",
    "    plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14)\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Normalized\n",
    "if sns:\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',\n",
    "                xticklabels=class_list, yticklabels=class_list, ax=axes[1])\n",
    "else:\n",
    "    im = axes[1].imshow(cm_normalized, cmap='Blues')\n",
    "    for i in range(len(class_list)):\n",
    "        for j in range(len(class_list)):\n",
    "            axes[1].text(j, i, f'{cm_normalized[i, j]:.3f}', ha='center', va='center')\n",
    "    axes[1].set_xticks(range(len(class_list)))\n",
    "    axes[1].set_yticks(range(len(class_list)))\n",
    "    axes[1].set_xticklabels(class_list, rotation=45)\n",
    "    axes[1].set_yticklabels(class_list)\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14)\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\n5. Classification Report (Pixel-level):\")\n",
    "print(classification_report(all_targets, all_predictions, target_names=class_list, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics per class\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x = np.arange(len(class_list))\n",
    "width = 0.35\n",
    "\n",
    "# Dice scores\n",
    "dice_stds = [np.std(all_dice_scores[i]) for i in range(len(class_list))]\n",
    "axes[0].bar(x, dice_means, width, yerr=dice_stds, capsize=5, color='steelblue', alpha=0.8)\n",
    "axes[0].set_ylabel('Dice Score')\n",
    "axes[0].set_title('Dice Score per Class')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(class_list, rotation=45, ha='right')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].axhline(y=overall_dice_mean, color='r', linestyle='--', label=f'Mean: {overall_dice_mean:.4f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# IoU scores\n",
    "iou_stds = [np.std(all_iou_scores[i]) for i in range(len(class_list))]\n",
    "axes[1].bar(x, iou_means, width, yerr=iou_stds, capsize=5, color='darkgreen', alpha=0.8)\n",
    "axes[1].set_ylabel('IoU')\n",
    "axes[1].set_title('IoU (Jaccard Index) per Class')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(class_list, rotation=45, ha='right')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].axhline(y=overall_iou_mean, color='r', linestyle='--', label=f'Mean: {overall_iou_mean:.4f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY - BREAST TUMOR SEGMENTATION MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel: DeepLabV3+ with EfficientNet-B5 encoder\")\n",
    "print(f\"Image Size: {img_size}x{img_size}\")\n",
    "print(f\"Number of Classes: {len(class_list)}\")\n",
    "print(f\"Test Set Size: {len(test_dataset)} images\")\n",
    "print(f\"\\n{'Metric':<25} {'Value':<15}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Pixel Accuracy':<25} {pixel_acc_mean:.4f} ± {np.std(pixel_accuracies):.4f}\")\n",
    "print(f\"{'Mean Dice Score (mDSC)':<25} {overall_dice_mean:.4f} ± {overall_dice_std:.4f}\")\n",
    "print(f\"{'Mean IoU (mIoU)':<25} {overall_iou_mean:.4f} ± {overall_iou_std:.4f}\")\n",
    "print(\"\\nClass-wise Dice Scores:\")\n",
    "for i, class_name in enumerate(class_list):\n",
    "    print(f\"  {class_name:<20} {dice_means[i]:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
